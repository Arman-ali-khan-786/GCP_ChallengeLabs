



-------------------------------------
Task 1: Run a simple Dataflow job
-------------------------------------


1.1.) Created a BigQuery dataset called lab

    In the Cloud Console, click on Navigation Menu > BigQuery.
    Select your project in the left pane.
    Click CREATE DATASET.

    Enter lab in the Dataset ID, then click Create dataset.
    Image in ☁ Perform Foundational Data, ML, and AI Tasks in Google Cloud: Challenge Lab | logbook

>> Run the following command :-
  
   gsutil cp gs://cloud-training/gsp323/lab.csv .
   cat lab.csv
   gsutil cp gs://cloud-training/gsp323/lab.schema .
   cat lab.schema

1.2.) Create a Cloud Storage bucket

    In the Cloud Console, click on Navigation Menu > Storage.
    Click CREATE BUCKET.
    Copy your GCP Project ID to Name your bucket.
    Click CREATE.

1.3 Create a Dataflow job

    click on Navigation Menu -->--> Dataflow.
    Click CREATE JOB FROM TEMPLATE.
    give an arbitrary job name.(for example "lab-job")

    From the dropdown under Dataflow template, select Text Files on Cloud Storage to BigQuery under “Process Data in Bulk (batch)”. 
    IMP!!!! --->>> (DO NOT select the item under “Process Data Continuously (stream)”).


    Under the Required parameters, enter the following values:
  ------------------------------------------|------------------------------------------
  |         Field                           |                   Value                 |
  ------------------------------------------|------------------------------------------                               
  | JavaScript UDF path in Cloud Storage    |	gs://cloud-training/gsp323/lab.js     |
  | JSON path 	                            |   gs://cloud-training/gsp323/lab.schema |
  | JavaScript UDF name 	            |   transform                             |
  | BigQuery output table 	            |   YOUR_PROJECT:lab.customers            |
  | Cloud Storage input path 	            |   gs://cloud-training/gsp323/lab.csv    |
  | Temporary BigQuery directory 	    |   gs://YOUR_PROJECT/bigquery_temp       |
  | Temporary location 	                    |   gs://YOUR_PROJECT/temp                |
  |-----------------------------------------|-----------------------------------------|
    <!-- Replace YOUR_PROJECT with your project ID. -->
    
  Click RUN JOB.

----------------------------------
Task 2: Run a simple Dataproc job
----------------------------------

Create a Dataproc cluster

    click on Navigation Menu > Dataproc > Clusters.
    Click CREATE CLUSTER.
    Make sure the cluster is going to create in the region us-central1.
    Click Create.

    After the cluster has been created, click the SSH button in the row of the master instance.

    In the SSH console, 

>>> run the following command:

    hdfs dfs -cp gs://cloud-training/gsp323/data.txt /data.txt

    Close the SSH window and go back to the Cloud Console.
    Click SUBMIT JOB on the cluster details page.
_
    Select Spark from the dropdown of “Job type”.
    Copy "org.apache.spark.examples.SparkPageRank"  to “Main class or jar”.
    Copy "file:///usr/lib/spark/examples/jars/spark-examples.jar" to “Jar files”.
    Enter "/data.txt" to “Arguments”.
   
    Click CREATE.
_

----------------------------------
Task 3: Run a simple Dataprep job
----------------------------------


    click on Navigation menu > Dataprep.

    After entering the home page of Cloud Dataprep, click the Import Data button.

    In the Import Data page, select GCS/Cloud storage in the left pane.
    Click on the pencil icon under Choose a file or folder.

    Copy "gs://cloud-training/gsp323/runs.csv" to the textbox
    then click the Go button next to it.

    After showing the preview of runs.csv in the right pane, click on the Import & Wrangle button.

>>> Transform data in Dataprep

    After launching the Dataperop Transformer, scroll right to the end and select column10.
    In the Details pane, click FAILURE under Unique Values to show the context menu.

    Select Delete rows with selected values to Remove all rows with the state of “FAILURE”.

    Click the downward arrow next to column9, choose Filter rows > On column value > Contains.

    In the Filter rows pane, enter the regex pattern /(^0$|^0\.0$)/ to “Pattern to match”.

    Select Delete matching rows under the Action section, then click the Add button.
    

    Rename the columns to be:
        runid      ---> col 2
        userid     ---> col 3
        labid      ---> col 4
        lab_title  ---> col 5
        start      ---> col 6
        end        ---> col 7
        time       ---> col 8
        score      ---> col 9
        state      ---> col 10

    Confirm the recipe should have 11 steps;

    Click Run Job.

-------------
Task 4: AI
-------------

   gcloud iam service-accounts create my-natlang-sa \
   >--display-name "my natural language service account"

   gcloud iam service-accounts keys create ~/key.json \
   >--iam-account my-natlang-sa@${GOOGLE_CLOUD_PROJECT}.iam.gserviceaccount.com

   export GOOGLE_APPLICATION_CREDENTIALS="/home/$USER/key.json"

   gcloud auth activate-service-account my-natlang-sa@${GOOGLE_CLOUD_PROJECT}.iam.gserviceaccount.com --key-file=$GOOGLE_APPLICATION_CREDENTIALS

   gcloud ml language analyze-entities --content="Old Norse texts portray Odin as one-eyed and long-bearded, frequently wielding a spear named Gungnir and wearing a cloak and a broad hat." > result.json

   gcloud auth login
     (Copy the token from the link provided)

   gsutil cp result.json gs://YOUR_PROJECT-marking/task4-cnl.result


   export API_KEY={Replace with API KEY}

   nano request.json

-->> replace this content with existing one;
{

  "config": {

      "encoding":"FLAC",

      "languageCode": "en-US"

  },

  "audio": {

      "uri":"gs://cloud-training/gsp323/task4.flac"

  }

}


  curl -s -X POST -H "Content-Type: application/json" --data-binary @request.json \
  >"https://speech.googleapis.com/v1/speech:recognize?key=${API_KEY}" > result.json

  gsutil cp result.json gs://YOUR_PROJECT-marking/task4-gcs.result

  gcloud iam service-accounts create quickstart
  gcloud iam service-accounts keys create key.json --iam-account quickstart@${GOOGLE_CLOUD_PROJECT}.iam.gserviceaccount.com
  
  gcloud auth activate-service-account --key-file key.json
  export ACCESS_TOKEN=$(gcloud auth print-access-token)

  nano request.json
--->> add this,
{

   "inputUri":"gs://spls/gsp154/video/chicago.mp4",

   "features": [

       "TEXT_DETECTION"

   ]

}

    curl -s -H 'Content-Type: application/json' \
    >-H "Authorization: Bearer $ACCESS_TOKEN" \
    >'https://videointelligence.googleapis.com/v1/videos:annotate' \
    >-d @request.json


    curl -s -H 'Content-Type: application/json' -H "Authorization: Bearer $ACCESS_TOKEN" 'https://videointelligence.googleapis.com/v1/operations/OPERATION_FROM_PREVIOUS_REQUEST' > result1.json
    gsutil cp result1.json gs://YOUR_PROJECT-marking/task4-gvi.result







----------------------------<<<<<<<<<<<<<<<<<<<<<<<<<___Congralutions___>>>>>>>>>>>>>>>>>>>>>>>----------------------------------